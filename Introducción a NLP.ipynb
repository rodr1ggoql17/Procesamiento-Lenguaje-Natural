{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNvxLE2aa2jB10gXevQPZZc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodr1ggoql17/Procesamiento-Lenguaje-Natural/blob/main/Introducci%C3%B3n%20a%20NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizar"
      ],
      "metadata": {
        "id": "NazoSFGe9Jh7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5I_W6_M9EkF"
      },
      "outputs": [],
      "source": [
        "texto = \"Hola, ¿Cómo Estás?\"\n",
        "tokens = texto.split()\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo anterior no sirve para problemas más complejos cuando se quiere hacer una completa comprensión del texto.\n",
        "Para ello existen distintas pespectivas para abordar el tema\n",
        "\n",
        "\n",
        "*   Basada en palabras (casa, perro, gato, etc)\n",
        "*   Basada en caracteres (a,d,4,#, etc)\n",
        "*   Basada en subplabras (automovil: auto, movil)\n",
        "\n"
      ],
      "metadata": {
        "id": "6SUawFI-9pkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Manejo de mayusculas y minusculas\n",
        "texto = \"Hola, ¿Cómo estás?\"\n",
        "texto = texto.lower()\n",
        "tokens = texto.split()\n",
        "tokens"
      ],
      "metadata": {
        "id": "eawkN1Q29lOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop Words (palabras de parada) son palabras que no aportan mucho al lenguaje y aumentan la dimensión del lenguaje"
      ],
      "metadata": {
        "id": "DM46ZIETAanL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"El gato es Negro y el perro es blanco\""
      ],
      "metadata": {
        "id": "RcBr8gOcAa_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importar nltk -> natural language toolkit\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "M44vsyjVBRjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "rbdJ2MIRB4hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "1TxjeF2HCmsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('spanish'))\n",
        "stop_words"
      ],
      "metadata": {
        "id": "Ftd-eNdhB-sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto = texto.lower()"
      ],
      "metadata": {
        "id": "SZR8yLVeFKeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(texto)\n",
        "tokens"
      ],
      "metadata": {
        "id": "2tH6ciXbD5cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_filtrado = [word for word in tokens if not word in stop_words]\n",
        "texto_filtrado"
      ],
      "metadata": {
        "id": "VUGcEx3pEwrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming y Lemmatization\n",
        "objetivo es reducir la dimensión de bad of words"
      ],
      "metadata": {
        "id": "CE09KoioG1Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "t3xPrSkMFUtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "#Crear el stemmer en español\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "#Probarlo con la palabra caminar\n",
        "print(stemmer.stem('caminando'))\n",
        "print(stemmer.stem('caminar'))\n",
        "print(stemmer.stem('caminó'))"
      ],
      "metadata": {
        "id": "Gz_fSj1THTmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# descargar paquetes para hacer la Lematización\n",
        "!pip install spacy -q\n",
        "!python -m spacy download es_core_news_sm -q"
      ],
      "metadata": {
        "id": "BVbUOdxkIFsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# cargar modelo en español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "#crear documento\n",
        "doc = nlp(\"caminar caminando caminó\")\n",
        "\n",
        "#imprimir el texto y el lema de cada token\n",
        "for token in doc:\n",
        "  print(token.text, \"-> \", token.lemma_)"
      ],
      "metadata": {
        "id": "qgGM_DQGIckD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observaciones:\n",
        "\n",
        "\n",
        "*   La lemmatizacion puede ser más efectiva que el stemming, pero también es más costosa computacionalmente.\n",
        "*   El uso de la lemmatization puede requerir etiquetado previo\n",
        "\n"
      ],
      "metadata": {
        "id": "0FBGHEZLJXwk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qk1ccNffJuVU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}