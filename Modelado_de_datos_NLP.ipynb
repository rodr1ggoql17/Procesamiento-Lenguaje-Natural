{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1w6W_sfW6wPC0s6V8oZcy4COXMj9IDEbO",
      "authorship_tag": "ABX9TyN9bN/E2AMgIt7H/zCrSwtQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodr1ggoql17/Procesamiento-Lenguaje-Natural/blob/main/Modelado_de_datos_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ejemplo tokenizacion simple"
      ],
      "metadata": {
        "id": "Lg0tRb__L4Ak"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8E9IrFkLqpT"
      },
      "outputs": [],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos el dataset con librería de pandas\n"
      ],
      "metadata": {
        "id": "ElFSM8-Tjk8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/CURSO NLP/data/df_total.csv', encoding='UTF-8')\n",
        "df"
      ],
      "metadata": {
        "id": "C2icztaxMEJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['news'][3]"
      ],
      "metadata": {
        "id": "mED3QwUEMTvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separar datos de entrada y etiquetas\n",
        "X = df['news']\n",
        "Y = df['Type']"
      ],
      "metadata": {
        "id": "SMZQF969Med8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizamos el tipo de noticias presentes en nuestros datos"
      ],
      "metadata": {
        "id": "MVV9GdBekORv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Type'].value_counts()"
      ],
      "metadata": {
        "id": "yEDGKkPAMtEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('La cantidad de tipos de noticias son : ', len(df['Type'].value_counts()))"
      ],
      "metadata": {
        "id": "fMhldgd2M2fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('La longitud del dataframe es: ', len(df))\n",
        "#df = df.drop(5)\n",
        "print('La longitud del dataframe ahora es: ', len(df))"
      ],
      "metadata": {
        "id": "awcfke-qbIOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['news']\n",
        "y = df['Type']"
      ],
      "metadata": {
        "id": "fGeUSh_rckAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vectorizar los datos de entrenamiento y prueba\n",
        "\n",
        "*   Como obs, este paso iba luego de separar los datos en conjuntos de entrenamiento y prueba, pero por alguna extraña razón la dimensión de los datos cambiaba por lo cual el modelo no soportaba realizar predicciones\n",
        "\n"
      ],
      "metadata": {
        "id": "HL_-sS4XkUlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "x_vectorizer = vectorizer.fit_transform(X)"
      ],
      "metadata": {
        "id": "kL27Xd85hvpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separar los el dataset en conjuntos de datos de entrenamiento y prueba"
      ],
      "metadata": {
        "id": "Cr3qBAAdd-8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_vectorizer,y, test_size = 0.2)"
      ],
      "metadata": {
        "id": "xggBtqO6bZ_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "NHXcgSX7c3rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "n11ZOuTLc9BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "qZrtrE-ddAGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "BlzTZEfrdBno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelo"
      ],
      "metadata": {
        "id": "OPOtzIrReNou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "model = MultinomialNB()"
      ],
      "metadata": {
        "id": "5mvCmM3ndoYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "03xPgwYGequX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "nBbXpCTVf_zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('La predicción del modelo es de: ', metrics.accuracy_score(y_test, y_pred)*100, \"%\")"
      ],
      "metadata": {
        "id": "Dy15hxc0gUW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "aLBaRBmPlq13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Librerias necesarias"
      ],
      "metadata": {
        "id": "VvBSyCGal_3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "SLbyFPCTltvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "D_UK0_iVmCFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creamos el stemmer\n",
        "stemmer = SnowballStemmer('spanish')"
      ],
      "metadata": {
        "id": "lqXFHYZ0mJEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_stem(text):\n",
        "  tokens = word_tokenize(text.lower()) # llevar el texto a minuscula\n",
        "  stems = [stemmer.stem(token) for token in tokens if token.isalpha()] # reducir la dimension\n",
        "  return ' '.join(stems)"
      ],
      "metadata": {
        "id": "hE3w0Kj8mJCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['new_stemmer'] = df['news'].apply(tokenize_and_stem)\n",
        "df['new_stemmer']"
      ],
      "metadata": {
        "id": "nqHaenzxmI_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['news'][0]"
      ],
      "metadata": {
        "id": "r__A-THYnfNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['new_stemmer'][0]"
      ],
      "metadata": {
        "id": "aldTuLFXnc4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelo con datos Stemmer"
      ],
      "metadata": {
        "id": "FhsWPHI4qH85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_stemmer = df['new_stemmer']\n",
        "vectorizer = CountVectorizer()\n",
        "x_stemmer = vectorizer.fit_transform(x_stemmer)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_stemmer, y, test_size=0.2)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train,y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('La predicción del modelo es de: ', metrics.accuracy_score(y_test, y_pred)*100, \"%\")"
      ],
      "metadata": {
        "id": "NFkhMMx9nzrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo mejoro ligeramente su rendimiento pero como se observa a continuación, con stemmer se redujo más de un 50% en la dimensión del vocabulario"
      ],
      "metadata": {
        "id": "6KG1rTmlp3Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_vectorizer"
      ],
      "metadata": {
        "id": "OzBIFnfepqph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_stemmer"
      ],
      "metadata": {
        "id": "f8EGwDaApnWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization"
      ],
      "metadata": {
        "id": "zgKwtjH1qMmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy -q\n",
        "!python -m spacy download es_core_news_sm -q"
      ],
      "metadata": {
        "id": "HXL6ESOyq3Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('es_core_news_sm')"
      ],
      "metadata": {
        "id": "EjMBshdfqFa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "  doc = nlp(text.lower()) # llevamos el texto a minuscula\n",
        "  lemmas = [token.lemma_ for token in doc if token.is_alpha] # aplicamos lemmatization a la noticia\n",
        "  return ' '.join(lemmas)"
      ],
      "metadata": {
        "id": "bADRPRaGqVLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['news_lemma'] = df['news'].apply(lemmatize_text)\n",
        "df['news_lemma']"
      ],
      "metadata": {
        "id": "fM3fj8MerLNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['news_lemma'][0]"
      ],
      "metadata": {
        "id": "4ERZuU9wrdus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_lemma = df['news_lemma']\n",
        "vectorizer = CountVectorizer()\n",
        "x_lemma = vectorizer.fit_transform(x_lemma)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_lemma, y, test_size=0.2)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train,y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('La predicción del modelo es de: ', metrics.accuracy_score(y_test, y_pred)*100, \"%\")"
      ],
      "metadata": {
        "id": "bDoNIaeKsfsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_lemma"
      ],
      "metadata": {
        "id": "Z9EXVeKktCvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_stemmer"
      ],
      "metadata": {
        "id": "DPb412p5tEyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_vectorizer"
      ],
      "metadata": {
        "id": "Bp4bFSbdtPCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusión\n",
        "\n",
        "Los modelos poseen rendimientos similares pero la reducción de dimensiones que realiza stemmer y lemma es muy significativo, considerando que estamos trabajando en un proyecto con pocos datos"
      ],
      "metadata": {
        "id": "m8JBMgx3td14"
      }
    }
  ]
}